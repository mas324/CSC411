{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code example demonstrates how to create a matrix implementation of c4e2_2level_learning_mnist. More context for this code example can be found in the section \"Single matrix\" in Appendix F in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first piece of code code is identical to the implementation in c4e2_2level_learning_mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "\n",
    "np.random.seed(7) # To make repeatable\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 20\n",
    "TRAIN_IMAGE_FILENAME = '../data/mnist/train-images-idx3-ubyte'\n",
    "TRAIN_LABEL_FILENAME = '../data/mnist/train-labels-idx1-ubyte'\n",
    "TEST_IMAGE_FILENAME = '../data/mnist/t10k-images-idx3-ubyte'\n",
    "TEST_LABEL_FILENAME = '../data/mnist/t10k-labels-idx1-ubyte'\n",
    "\n",
    "# Function to read dataset.\n",
    "def read_mnist():\n",
    "    train_images = idx2numpy.convert_from_file(\n",
    "        TRAIN_IMAGE_FILENAME)\n",
    "    train_labels = idx2numpy.convert_from_file(\n",
    "        TRAIN_LABEL_FILENAME)\n",
    "    test_images = idx2numpy.convert_from_file(\n",
    "        TEST_IMAGE_FILENAME)\n",
    "    test_labels = idx2numpy.convert_from_file(\n",
    "        TEST_LABEL_FILENAME)\n",
    "\n",
    "    # Reformat and standardize.\n",
    "    x_train = train_images.reshape(60000, 784)\n",
    "    mean = np.mean(x_train)\n",
    "    stddev = np.std(x_train)\n",
    "    x_train = (x_train - mean) / stddev\n",
    "    x_test = test_images.reshape(10000, 784)\n",
    "    x_test = (x_test - mean) / stddev\n",
    "\n",
    "    # One-hot encoded output.\n",
    "    y_train = np.zeros((60000, 10))\n",
    "    y_test = np.zeros((10000, 10))\n",
    "    for i, y in enumerate(train_labels):\n",
    "        y_train[i][y] = 1\n",
    "    for i, y in enumerate(test_labels):\n",
    "        y_test[i][y] = 1\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Read train and test examples.\n",
    "x_train, y_train, x_test, y_test = read_mnist()\n",
    "index_list = list(range(len(x_train))) # Used for random order\n",
    "\n",
    "def layer_w(neuron_count, input_count):\n",
    "    weights = np.zeros((neuron_count, input_count+1))\n",
    "    for i in range(neuron_count):\n",
    "        for j in range(1, (input_count+1)):\n",
    "            weights[i][j] = np.random.uniform(-0.1, 0.1)\n",
    "    return weights\n",
    "\n",
    "# Declare matrices and vectors representing the neurons.\n",
    "hidden_layer_w = layer_w(25, 784)\n",
    "hidden_layer_y = np.zeros(25)\n",
    "hidden_layer_error = np.zeros(25)\n",
    "\n",
    "output_layer_w = layer_w(10, 25)\n",
    "output_layer_y = np.zeros(10)\n",
    "output_layer_error = np.zeros(10)\n",
    "\n",
    "chart_x = []\n",
    "chart_y_train = []\n",
    "chart_y_test = []\n",
    "def show_learning(epoch_no, train_acc, test_acc):\n",
    "    global chart_x\n",
    "    global chart_y_train\n",
    "    global chart_y_test\n",
    "    print('epoch no:', epoch_no, ', train_acc: ',\n",
    "          '%6.4f' % train_acc,\n",
    "          ', test_acc: ', '%6.4f' % test_acc)\n",
    "    chart_x.append(epoch_no + 1)\n",
    "    chart_y_train.append(1.0 - train_acc)\n",
    "    chart_y_test.append(1.0 - test_acc)\n",
    "\n",
    "def plot_learning():\n",
    "    plt.plot(chart_x, chart_y_train, 'r-',\n",
    "             label='training error')\n",
    "    plt.plot(chart_x, chart_y_test, 'b-',\n",
    "             label='test error')\n",
    "    plt.axis([0, len(chart_x), 0.0, 1.0])\n",
    "    plt.xlabel('training epochs')\n",
    "    plt.ylabel('error')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions that have changed compared to c4e2_2level_learning_mnist are forward_pass, backward_pass, and adjust_weights. In these functions, we no longer loop over the individual neurons and do dot products, but instead, we handle an entire layer in parallel using matrix operations.\n",
    "\n",
    "The forward_pass function is straightforward. We use the NumPy matmul function to multiply the weight matrix by the input vector and then apply the activation function tanh on the resulting output vector. We then append a bias needed for the output layer using the concatenate function and do the matrix multiplication and activation function for the output layer as well.\n",
    "\n",
    "The backward_pass function is not much more complicated. We compute the derivatives of the error function and the activation function but note that all these computations are done on vectors (i.e., all neurons in parallel). Another thing to note is that the mathematical operators +, -, and * are elementwise operators. That is, there is a big difference between using * and the matmul function. One thing to note is the call to np.matrix.transpose and the indexing we do with output_layer_w[:, 1:]. The transpose operation is needed to make the dimensions of the weight matrix match what is needed for a matrix multiplication with the error vector. The indexing is done to get rid of the bias weights when computing the error terms for the hidden neurons because the bias weight from the output layer is not needed for that operation.\n",
    "\n",
    "The adjust_weights function is slightly tricky. For each of the two layers, we need to create a matrix with the same dimensions as the weight matrix for that layer but where the elements represent the delta to subtract from the weights. The elements of this delta matrix are obtained by multiplying the input value that feeds into a weight by the error term for the neuron that the weight connects to and finally multiplying by the learning rate. We already have the error terms arranged in the vectors hidden_layer_error and output_layer_error. Similarly, we have the input values for the two layers arranged in the vectors x and hidden_layer_y. For each layer we now combine the input vector with the error vector using the function np.outer which computes the outer product of the two vectors. It results in a matrix where the elements are all the pairwise products from the elements in the two vectors, which is exactly what we want. We multiply the matrix by the learning rate and then subtract from the weight matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    global hidden_layer_y\n",
    "    global output_layer_y\n",
    "    # Activation function for hidden layer.\n",
    "    hidden_layer_z = np.matmul(hidden_layer_w, x)\n",
    "    hidden_layer_y = np.tanh(hidden_layer_z)\n",
    "    hidden_output_array = np.concatenate(\n",
    "        (np.array([1.0]), hidden_layer_y))\n",
    "    # Activation function for output layer.\n",
    "    output_layer_z = np.matmul(output_layer_w,\n",
    "        hidden_output_array)\n",
    "    output_layer_y = 1.0 / (1.0 + np.exp(-output_layer_z))\n",
    "\n",
    "def backward_pass(y_truth):\n",
    "    global hidden_layer_error\n",
    "    global output_layer_error\n",
    "    # Backpropagate error for each output neuron.\n",
    "    error_prime = -(y_truth - output_layer_y)\n",
    "    output_log_prime = output_layer_y * (\n",
    "        1.0 - output_layer_y)\n",
    "    output_layer_error = error_prime * output_log_prime\n",
    "    # Backpropagate error for each hidden neuron.\n",
    "    hidden_tanh_prime = 1.0 - hidden_layer_y**2\n",
    "    hidden_weighted_error = np.matmul(np.matrix.transpose(\n",
    "        output_layer_w[:, 1:]), output_layer_error)\n",
    "    hidden_layer_error = (\n",
    "        hidden_tanh_prime * hidden_weighted_error)\n",
    "\n",
    "def adjust_weights(x):\n",
    "    global output_layer_w\n",
    "    global hidden_layer_w\n",
    "    delta_matrix = np.outer(\n",
    "        hidden_layer_error, x) * LEARNING_RATE\n",
    "    hidden_layer_w -= delta_matrix\n",
    "    hidden_output_array = np.concatenate(\n",
    "        (np.array([1.0]), hidden_layer_y))\n",
    "    delta_matrix = np.outer(\n",
    "        output_layer_error,\n",
    "        hidden_output_array) * LEARNING_RATE\n",
    "    output_layer_w -= delta_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network training loop is unchanged compared to c4e2_2level_learning_mnist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network training loop.\n",
    "for i in range(EPOCHS): # Train EPOCHS iterations\n",
    "    np.random.shuffle(index_list) # Randomize order\n",
    "    correct_training_results = 0\n",
    "    for j in index_list: # Train on all examples\n",
    "        x = np.concatenate((np.array([1.0]), x_train[j]))\n",
    "        forward_pass(x)\n",
    "        if output_layer_y.argmax() == y_train[j].argmax():\n",
    "            correct_training_results += 1\n",
    "        backward_pass(y_train[j])\n",
    "        adjust_weights(x)\n",
    "\n",
    "    correct_test_results = 0\n",
    "    for j in range(len(x_test)): # Evaluate network\n",
    "        x = np.concatenate((np.array([1.0]), x_test[j]))\n",
    "        forward_pass(x)\n",
    "        if output_layer_y.argmax() == y_test[j].argmax():\n",
    "            correct_test_results += 1\n",
    "    # Show progress.\n",
    "    show_learning(i, correct_training_results/len(x_train),\n",
    "                  correct_test_results/len(x_test))\n",
    "plot_learning() # Create plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
